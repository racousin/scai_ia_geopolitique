{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP - RAG (Retrieval-Augmented Generation) appliqu√© √† la G√©opolitique\n",
    "\n",
    "## Dur√©e : 1h30-2h\n",
    "\n",
    "### Objectifs p√©dagogiques\n",
    "- Comprendre l'architecture et le fonctionnement d'un syst√®me RAG\n",
    "- Impl√©menter un RAG simple pour l'analyse g√©opolitique\n",
    "- Explorer l'impact des diff√©rents param√®tres (chunking, embedding, retrieval)\n",
    "- Analyser les avantages et limites du RAG pour l'analyse documentaire\n",
    "\n",
    "### Pr√©requis\n",
    "- Avoir ex√©cut√© le script de r√©cup√©ration de documents\n",
    "- Connaissances de base sur les embeddings (TP pr√©c√©dent)\n",
    "- Notions de prompting et LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction au RAG\n",
    "\n",
    "### Qu'est-ce que le RAG ?\n",
    "\n",
    "**RAG = Retrieval-Augmented Generation**\n",
    "\n",
    "Le RAG combine:\n",
    "1. **Retrieval** (Recherche) : Trouver les documents pertinents dans une base de donn√©es\n",
    "2. **Augmented** (Augment√©) : Enrichir le contexte du LLM avec ces documents\n",
    "3. **Generation** (G√©n√©ration) : Produire une r√©ponse bas√©e sur le contexte enrichi\n",
    "\n",
    "### Pourquoi utiliser le RAG ?\n",
    "\n",
    "‚úÖ **Actualit√©** : Acc√®s √† des informations r√©centes non pr√©sentes dans le LLM\n",
    "‚úÖ **Pr√©cision** : R√©ponses bas√©es sur des sources sp√©cifiques\n",
    "‚úÖ **Tra√ßabilit√©** : Possibilit√© de citer les sources\n",
    "‚úÖ **Personnalisation** : Utilisation de documents sp√©cifiques √† un domaine\n",
    "\n",
    "### Architecture simplifi√©e\n",
    "\n",
    "```\n",
    "Question ‚Üí Embedding ‚Üí Recherche ‚Üí Documents pertinents\n",
    "                                           ‚Üì\n",
    "                                    LLM + Contexte ‚Üí R√©ponse\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Installation et configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des packages n√©cessaires\n",
    "!pip install langchain langchain-community langchain-huggingface\n",
    "!pip install sentence-transformers transformers\n",
    "!pip install chromadb faiss-cpu\n",
    "!pip install pandas numpy\n",
    "!pip install openai anthropic  # Pour les LLMs (optionnel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports essentiels\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict\n",
    "\n",
    "# LangChain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma, FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Transformers\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úì Packages import√©s avec succ√®s!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Chargement des documents g√©opolitiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration des chemins\n",
    "DOCS_FOLDER = \"documents_geopolitique\"\n",
    "\n",
    "def charger_documents():\n",
    "    \"\"\"Charge tous les documents disponibles\"\"\"\n",
    "    documents = []\n",
    "    \n",
    "    # 1. Charger les articles JSON\n",
    "    json_files = [f for f in os.listdir(DOCS_FOLDER) if f.endswith('.json')]\n",
    "    if json_files:\n",
    "        with open(os.path.join(DOCS_FOLDER, json_files[0]), 'r', encoding='utf-8') as f:\n",
    "            articles = json.load(f)\n",
    "            for article in articles:\n",
    "                doc = Document(\n",
    "                    page_content=f\"Titre: {article['titre']}\\n\\n{article['contenu']}\",\n",
    "                    metadata={\n",
    "                        \"source\": article['source'],\n",
    "                        \"titre\": article['titre'],\n",
    "                        \"langue\": article['langue'],\n",
    "                        \"type\": \"actualite\"\n",
    "                    }\n",
    "                )\n",
    "                documents.append(doc)\n",
    "    \n",
    "    # 2. Charger les documents de r√©f√©rence\n",
    "    ref_folder = os.path.join(DOCS_FOLDER, \"documents_reference\")\n",
    "    if os.path.exists(ref_folder):\n",
    "        for filename in os.listdir(ref_folder):\n",
    "            if filename.endswith('.txt'):\n",
    "                with open(os.path.join(ref_folder, filename), 'r', encoding='utf-8') as f:\n",
    "                    content = f.read()\n",
    "                    doc = Document(\n",
    "                        page_content=content,\n",
    "                        metadata={\n",
    "                            \"source\": \"reference\",\n",
    "                            \"filename\": filename,\n",
    "                            \"type\": \"reference\"\n",
    "                        }\n",
    "                    )\n",
    "                    documents.append(doc)\n",
    "    \n",
    "    print(f\"‚úì {len(documents)} documents charg√©s\")\n",
    "    return documents\n",
    "\n",
    "# Charger les documents\n",
    "documents = charger_documents()\n",
    "\n",
    "# Aper√ßu\n",
    "if documents:\n",
    "    print(f\"\\nExemple de document:\")\n",
    "    print(f\"Contenu: {documents[0].page_content[:200]}...\")\n",
    "    print(f\"M√©tadonn√©es: {documents[0].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. √âtape 1 : Text Splitting (D√©coupage des documents)\n",
    "\n",
    "Le d√©coupage est crucial : des chunks trop grands = moins de pr√©cision, trop petits = perte de contexte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration du text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,        # üìù TODO: Essayez 200, 500, 1000\n",
    "    chunk_overlap=50,      # üìù TODO: Essayez 0, 50, 100\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "# D√©couper les documents\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "print(f\"‚úì {len(documents)} documents d√©coup√©s en {len(chunks)} chunks\")\n",
    "print(f\"\\nTaille moyenne des chunks: {np.mean([len(chunk.page_content) for chunk in chunks]):.0f} caract√®res\")\n",
    "\n",
    "# Visualiser quelques chunks\n",
    "print(\"\\nExemples de chunks:\")\n",
    "for i, chunk in enumerate(chunks[:3]):\n",
    "    print(f\"\\n--- Chunk {i+1} ---\")\n",
    "    print(f\"Taille: {len(chunk.page_content)} caract√®res\")\n",
    "    print(f\"Contenu: {chunk.page_content[:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü§î Question 1 : Impact du chunking\n",
    "Modifiez `chunk_size` et `chunk_overlap` dans la cellule ci-dessus. Comment cela affecte-t-il :\n",
    "- Le nombre total de chunks ?\n",
    "- La coh√©rence du contenu dans chaque chunk ?\n",
    "- Quel compromis devez-vous faire ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. √âtape 2 : Embeddings et Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choix du mod√®le d'embeddings\n",
    "EMBEDDING_MODELS = {\n",
    "    \"multilingual-mini\": \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "    \"multilingual-mpnet\": \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\",\n",
    "    \"french-camembert\": \"dangvantuan/sentence-camembert-base\",\n",
    "    \"english-minilm\": \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "}\n",
    "\n",
    "# üìù TODO: Changez le mod√®le ici\n",
    "model_choice = \"multilingual-mini\"\n",
    "\n",
    "# Cr√©er les embeddings\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=EMBEDDING_MODELS[model_choice],\n",
    "    model_kwargs={'device': 'cpu'},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "print(f\"‚úì Mod√®le d'embeddings charg√©: {model_choice}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er le vector store\n",
    "# üìù TODO: Essayez 'chroma' ou 'faiss'\n",
    "vector_store_type = \"faiss\"  \n",
    "\n",
    "print(f\"Cr√©ation du vector store ({vector_store_type})...\")\n",
    "\n",
    "if vector_store_type == \"chroma\":\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=chunks,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=\"./chroma_db\"\n",
    "    )\n",
    "else:  # faiss\n",
    "    vectorstore = FAISS.from_documents(\n",
    "        documents=chunks,\n",
    "        embedding=embeddings\n",
    "    )\n",
    "\n",
    "print(f\"‚úì Vector store cr√©√© avec {len(chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. √âtape 3 : Test de la recherche (Retrieval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tester_recherche(query: str, k: int = 3):\n",
    "    \"\"\"Teste la recherche de documents similaires\"\"\"\n",
    "    print(f\"\\nüîç Recherche: '{query}'\")\n",
    "    print(f\"Top {k} r√©sultats:\\n\")\n",
    "    \n",
    "    # Recherche\n",
    "    resultats = vectorstore.similarity_search_with_score(query, k=k)\n",
    "    \n",
    "    for i, (doc, score) in enumerate(resultats):\n",
    "        print(f\"--- R√©sultat {i+1} (score: {score:.3f}) ---\")\n",
    "        print(f\"Source: {doc.metadata}\")\n",
    "        print(f\"Extrait: {doc.page_content[:200]}...\\n\")\n",
    "    \n",
    "    return resultats\n",
    "\n",
    "# Test avec diff√©rentes requ√™tes\n",
    "queries_test = [\n",
    "    \"Relations entre la Chine et les √âtats-Unis\",\n",
    "    \"Changement climatique et g√©opolitique\",\n",
    "    \"Conflits en Afrique\"\n",
    "]\n",
    "\n",
    "# üìù TODO: Modifiez k (nombre de r√©sultats) - essayez 1, 3, 5\n",
    "k_resultats = 3\n",
    "\n",
    "for query in queries_test[:1]:  # Tester la premi√®re requ√™te\n",
    "    resultats = tester_recherche(query, k=k_resultats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü§î Question 2 : Qualit√© de la recherche\n",
    "- Les documents retrouv√©s sont-ils pertinents ?\n",
    "- Comment le nombre de r√©sultats (k) affecte-t-il la qualit√© ?\n",
    "- Testez avec vos propres questions g√©opolitiques !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìù TODO: Testez vos propres questions ici\n",
    "ma_question = \"Quel est le r√¥le de l'Union europ√©enne dans les conflits actuels ?\"\n",
    "mes_resultats = tester_recherche(ma_question, k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. √âtape 4 : Configuration du LLM pour la g√©n√©ration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour ce TP, nous utilisons un petit mod√®le open-source\n",
    "# Note: Pour de meilleurs r√©sultats, utilisez GPT-3.5/4 ou Claude avec une API key\n",
    "\n",
    "def creer_llm_simple():\n",
    "    \"\"\"Cr√©e un LLM simple pour la g√©n√©ration\"\"\"\n",
    "    \n",
    "    # Utiliser un mod√®le l√©ger\n",
    "    model_id = \"google/flan-t5-base\"  # üìù TODO: Essayez \"google/flan-t5-small\" ou \"google/flan-t5-large\"\n",
    "    \n",
    "    print(f\"Chargement du mod√®le {model_id}...\")\n",
    "    \n",
    "    # Pipeline de g√©n√©ration\n",
    "    pipe = pipeline(\n",
    "        \"text2text-generation\",\n",
    "        model=model_id,\n",
    "        max_length=512,\n",
    "        temperature=0.7,  # üìù TODO: Essayez 0.1 (d√©terministe) √† 1.0 (cr√©atif)\n",
    "        do_sample=True\n",
    "    )\n",
    "    \n",
    "    # Wrapper pour LangChain\n",
    "    llm = HuggingFacePipeline(pipeline=pipe)\n",
    "    \n",
    "    print(\"‚úì LLM charg√© et pr√™t!\")\n",
    "    return llm\n",
    "\n",
    "# Cr√©er le LLM\n",
    "llm = creer_llm_simple()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Assemblage du syst√®me RAG complet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template de prompt pour le RAG\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# üìù TODO: Modifiez ce template selon vos besoins\n",
    "template = \"\"\"Utilise les extraits de documents suivants pour r√©pondre √† la question. \n",
    "Si tu ne peux pas r√©pondre bas√© sur les documents, dis-le clairement.\n",
    "\n",
    "Documents:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "R√©ponse concise et factuelle:\"\"\"\n",
    "\n",
    "QA_PROMPT = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# Cr√©er la cha√Æne RAG\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",  # üìù TODO: Essayez aussi \"map_reduce\" pour de longs documents\n",
    "    retriever=vectorstore.as_retriever(\n",
    "        search_kwargs={\"k\": 3}  # üìù TODO: Ajustez le nombre de documents\n",
    "    ),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": QA_PROMPT}\n",
    ")\n",
    "\n",
    "print(\"‚úì Syst√®me RAG configur√© et pr√™t!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Test du syst√®me RAG complet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poser_question_rag(question: str, afficher_sources: bool = True):\n",
    "    \"\"\"Pose une question au syst√®me RAG\"\"\"\n",
    "    print(f\"\\nüí¨ Question: {question}\")\n",
    "    print(\"Recherche et g√©n√©ration en cours...\\n\")\n",
    "    \n",
    "    # Obtenir la r√©ponse\n",
    "    resultat = qa_chain({\"query\": question})\n",
    "    \n",
    "    # Afficher la r√©ponse\n",
    "    print(\"üìù R√©ponse:\")\n",
    "    print(resultat['result'])\n",
    "    \n",
    "    # Afficher les sources\n",
    "    if afficher_sources and 'source_documents' in resultat:\n",
    "        print(\"\\nüìö Sources utilis√©es:\")\n",
    "        for i, doc in enumerate(resultat['source_documents']):\n",
    "            print(f\"\\n  Source {i+1}:\")\n",
    "            print(f\"  - Type: {doc.metadata.get('type', 'inconnu')}\")\n",
    "            print(f\"  - Source: {doc.metadata.get('source', 'inconnue')}\")\n",
    "            print(f\"  - Extrait: {doc.page_content[:100]}...\")\n",
    "    \n",
    "    return resultat\n",
    "\n",
    "# Questions de test\n",
    "questions_geopolitiques = [\n",
    "    \"Quels sont les principaux conflits r√©gionaux actuels ?\",\n",
    "    \"Comment le changement climatique affecte-t-il la g√©opolitique ?\",\n",
    "    \"Quel est le r√¥le des BRICS dans le syst√®me international ?\",\n",
    "    \"Quelles sont les tensions en mer de Chine ?\"\n",
    "]\n",
    "\n",
    "# Tester une question\n",
    "resultat = poser_question_rag(questions_geopolitiques[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü§î Question 3 : Analyse de la g√©n√©ration\n",
    "- La r√©ponse est-elle coh√©rente avec les sources ?\n",
    "- Y a-t-il des hallucinations (informations invent√©es) ?\n",
    "- Comment am√©liorer la qualit√© des r√©ponses ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìù TODO: Testez d'autres questions\n",
    "for question in questions_geopolitiques[1:3]:\n",
    "    resultat = poser_question_rag(question, afficher_sources=False)\n",
    "    print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Exp√©rimentations avanc√©es"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 1 : Comparaison avec/sans RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparer_avec_sans_rag(question: str):\n",
    "    \"\"\"Compare les r√©ponses avec et sans RAG\"\"\"\n",
    "    print(f\"\\nüî¨ Comparaison pour: '{question}'\\n\")\n",
    "    \n",
    "    # Sans RAG (LLM seul)\n",
    "    print(\"1Ô∏è‚É£ SANS RAG (LLM seul):\")\n",
    "    reponse_sans_rag = llm(question)\n",
    "    print(reponse_sans_rag)\n",
    "    \n",
    "    # Avec RAG\n",
    "    print(\"\\n2Ô∏è‚É£ AVEC RAG (LLM + Documents):\")\n",
    "    resultat_rag = qa_chain({\"query\": question})\n",
    "    print(resultat_rag['result'])\n",
    "    \n",
    "    print(\"\\nSources RAG:\")\n",
    "    for doc in resultat_rag['source_documents'][:2]:\n",
    "        print(f\"- {doc.metadata.get('titre', 'Sans titre')[:50]}...\")\n",
    "\n",
    "# Test sur une question d'actualit√©\n",
    "question_test = \"Quelles sont les derni√®res tensions entre pays en 2025 ?\"\n",
    "comparer_avec_sans_rag(question_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 2 : Impact des param√®tres de recherche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyser_impact_k(question: str, k_values: list = [1, 3, 5, 10]):\n",
    "    \"\"\"Analyse l'impact du nombre de documents r√©cup√©r√©s\"\"\"\n",
    "    print(f\"\\nüìä Analyse de l'impact de k pour: '{question}'\\n\")\n",
    "    \n",
    "    for k in k_values:\n",
    "        print(f\"\\n--- k = {k} documents ---\")\n",
    "        \n",
    "        # Cr√©er un nouveau retriever avec k diff√©rent\n",
    "        qa_chain_k = RetrievalQA.from_chain_type(\n",
    "            llm=llm,\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=vectorstore.as_retriever(search_kwargs={\"k\": k}),\n",
    "            return_source_documents=True,\n",
    "            chain_type_kwargs={\"prompt\": QA_PROMPT}\n",
    "        )\n",
    "        \n",
    "        # Obtenir la r√©ponse\n",
    "        resultat = qa_chain_k({\"query\": question})\n",
    "        \n",
    "        print(f\"R√©ponse ({len(resultat['result'])} caract√®res): {resultat['result'][:150]}...\")\n",
    "        print(f\"Nombre de sources uniques: {len(set(doc.metadata.get('source', '') for doc in resultat['source_documents']))}\")\n",
    "\n",
    "# Analyser\n",
    "analyser_impact_k(\"Quels sont les enjeux de l'intelligence artificielle en g√©opolitique ?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü§î Question 4 : Optimisation du RAG\n",
    "Bas√© sur vos exp√©rimentations :\n",
    "- Quel nombre de documents (k) donne les meilleurs r√©sultats ?\n",
    "- Comment la taille des chunks affecte-t-elle la qualit√© ?\n",
    "- Quel mod√®le d'embedding est le plus adapt√© √† vos documents ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 3 : RAG multilingue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test avec des questions dans diff√©rentes langues\n",
    "questions_multilingues = {\n",
    "    \"fr\": \"Quelles sont les relations entre la France et l'Afrique ?\",\n",
    "    \"en\": \"What are the main challenges facing the United Nations?\",\n",
    "    \"es\": \"¬øCu√°l es el papel de Am√©rica Latina en la geopol√≠tica mundial?\"\n",
    "}\n",
    "\n",
    "print(\"üåç Test multilingue du RAG\\n\")\n",
    "\n",
    "for langue, question in questions_multilingues.items():\n",
    "    print(f\"\\n[{langue.upper()}] {question}\")\n",
    "    resultat = qa_chain({\"query\": question})\n",
    "    print(f\"R√©ponse: {resultat['result'][:200]}...\")\n",
    "    print(f\"Langues des sources: {set(doc.metadata.get('langue', 'inconnue') for doc in resultat['source_documents'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Analyse critique et limites du RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©ons un cas probl√©matique pour illustrer les limites\n",
    "def tester_limites_rag():\n",
    "    \"\"\"Teste les limites du syst√®me RAG\"\"\"\n",
    "    \n",
    "    cas_limites = [\n",
    "        {\n",
    "            \"type\": \"Question hors corpus\",\n",
    "            \"question\": \"Quelle est la politique spatiale du Luxembourg ?\"\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"Question n√©cessitant du raisonnement\",\n",
    "            \"question\": \"Si les tensions augmentent en mer de Chine et que le p√©trole devient rare, quel pays sera le plus affect√© ?\"\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"Question temporelle\",\n",
    "            \"question\": \"Comment ont √©volu√© les relations sino-am√©ricaines depuis 10 ans ?\"\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"Question contradictoire\",\n",
    "            \"question\": \"Pourquoi la Suisse est-elle membre de l'OTAN ?\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(\"üö® Test des limites du RAG\\n\")\n",
    "    \n",
    "    for cas in cas_limites:\n",
    "        print(f\"\\n--- {cas['type']} ---\")\n",
    "        print(f\"Question: {cas['question']}\")\n",
    "        \n",
    "        resultat = qa_chain({\"query\": cas['question']})\n",
    "        print(f\"\\nR√©ponse RAG: {resultat['result']}\")\n",
    "        print(f\"Nb sources trouv√©es: {len(resultat['source_documents'])}\")\n",
    "        \n",
    "        # Analyser la pertinence\n",
    "        if resultat['source_documents']:\n",
    "            premier_doc = resultat['source_documents'][0].page_content[:100]\n",
    "            print(f\"Pertinence de la 1√®re source: {premier_doc}...\")\n",
    "\n",
    "# Ex√©cuter les tests\n",
    "tester_limites_rag()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü§î Question 5 : R√©flexion sur les limites\n",
    "D'apr√®s vos observations :\n",
    "- Quelles sont les principales limites du RAG ?\n",
    "- Comment le syst√®me g√®re-t-il l'absence d'information ?\n",
    "- Peut-on faire confiance au RAG pour l'analyse g√©opolitique ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Synth√®se et bonnes pratiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìù TODO: Compl√©tez vos observations\n",
    "\n",
    "mes_conclusions_rag = {\n",
    "    \"avantages_observes\": [\n",
    "        \"Acc√®s √† des informations actualis√©es\",\n",
    "        # Ajoutez vos observations...\n",
    "    ],\n",
    "    \"limites_identifiees\": [\n",
    "        \"D√©pendance √† la qualit√© des documents sources\",\n",
    "        # Ajoutez vos observations...\n",
    "    ],\n",
    "    \"parametres_optimaux\": {\n",
    "        \"chunk_size\": 500,  # Votre choix\n",
    "        \"k_documents\": 3,   # Votre choix\n",
    "        \"modele_embedding\": \"multilingual-mini\",  # Votre choix\n",
    "    },\n",
    "    \"cas_usage_pertinents\": [\n",
    "        \"Analyse d'actualit√©s g√©opolitiques\",\n",
    "        # Ajoutez vos id√©es...\n",
    "    ],\n",
    "    \"ameliorations_possibles\": [\n",
    "        \"Utiliser un LLM plus puissant\",\n",
    "        # Ajoutez vos suggestions...\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"=== SYNTH√àSE DE MES OBSERVATIONS SUR LE RAG ===\")\n",
    "for categorie, contenu in mes_conclusions_rag.items():\n",
    "    print(f\"\\n{categorie.replace('_', ' ').upper()}:\")\n",
    "    if isinstance(contenu, list):\n",
    "        for item in contenu:\n",
    "            print(f\"  ‚Ä¢ {item}\")\n",
    "    elif isinstance(contenu, dict):\n",
    "        for key, value in contenu.items():\n",
    "            print(f\"  ‚Ä¢ {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Pour aller plus loin\n",
    "\n",
    "### Extensions possibles du RAG\n",
    "\n",
    "1. **RAG hybride** : Combiner recherche par mots-cl√©s et recherche s√©mantique\n",
    "2. **Re-ranking** : R√©ordonner les documents r√©cup√©r√©s selon leur pertinence\n",
    "3. **Multi-modal RAG** : Int√©grer images, graphiques, cartes\n",
    "4. **RAG conversationnel** : Maintenir un historique de conversation\n",
    "5. **RAG avec citations** : Citer pr√©cis√©ment les sources dans la r√©ponse\n",
    "\n",
    "### Code bonus : RAG avec m√©ta-donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple de recherche avec filtrage par m√©ta-donn√©es\n",
    "def recherche_avec_filtres(question: str, type_doc: str = None, langue: str = None):\n",
    "    \"\"\"Recherche avec filtrage sur les m√©ta-donn√©es\"\"\"\n",
    "    print(f\"\\nüîç Recherche filtr√©e: '{question}'\")\n",
    "    if type_doc:\n",
    "        print(f\"   Filtre type: {type_doc}\")\n",
    "    if langue:\n",
    "        print(f\"   Filtre langue: {langue}\")\n",
    "    \n",
    "    # Cr√©er un filtre (d√©pend du vector store utilis√©)\n",
    "    # Note: Ceci est un exemple conceptuel\n",
    "    resultats = vectorstore.similarity_search(\n",
    "        question,\n",
    "        k=5,\n",
    "        # filter={\"type\": type_doc} if type_doc else None  # Pseudo-code\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n{len(resultats)} r√©sultats trouv√©s\")\n",
    "    for i, doc in enumerate(resultats[:3]):\n",
    "        print(f\"\\n{i+1}. {doc.metadata}\")\n",
    "        print(f\"   {doc.page_content[:100]}...\")\n",
    "\n",
    "# Test\n",
    "recherche_avec_filtres(\n",
    "    \"Tensions internationales\",\n",
    "    type_doc=\"actualite\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions finales de r√©flexion\n",
    "\n",
    "### üéØ Pour votre pratique future :\n",
    "\n",
    "1. **Application professionnelle** : Comment pourriez-vous utiliser le RAG dans votre domaine d'expertise en g√©opolitique ?\n",
    "\n",
    "2. **√âthique et biais** : Comment s'assurer que le RAG ne propage pas de d√©sinformation ou de biais g√©opolitiques ?\n",
    "\n",
    "3. **Souverainet√© des donn√©es** : Quels enjeux pose l'utilisation de RAG avec des documents sensibles ou confidentiels ?\n",
    "\n",
    "4. **√âvolution future** : Comment imaginez-vous l'√©volution du RAG pour l'analyse g√©opolitique dans 5 ans ?\n",
    "\n",
    "5. **Alternatives** : Quelles autres approches pourrait-on combiner avec le RAG pour am√©liorer l'analyse ?\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Ressources compl√©mentaires\n",
    "\n",
    "1. **Documentation**\n",
    "   - [LangChain RAG Tutorial](https://python.langchain.com/docs/use_cases/question_answering)\n",
    "   - [Hugging Face RAG Guide](https://huggingface.co/docs/transformers/model_doc/rag)\n",
    "\n",
    "2. **Articles de recherche**\n",
    "   - \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\" (Lewis et al., 2020)\n",
    "   - \"REALM: Retrieval-Augmented Language Model Pre-Training\" (Guu et al., 2020)\n",
    "\n",
    "3. **Outils avanc√©s**\n",
    "   - [Haystack](https://haystack.deepset.ai/) - Framework NLP pour RAG\n",
    "   - [Weaviate](https://weaviate.io/) - Vector database sp√©cialis√©e\n",
    "   - [Pinecone](https://www.pinecone.io/) - Vector database cloud\n",
    "\n",
    "---\n",
    "\n",
    "**F√©licitations !** Vous avez maintenant une compr√©hension pratique du RAG appliqu√© √† la g√©opolitique. üéì\n",
    "\n",
    "N'h√©sitez pas √† exp√©rimenter davantage et √† adapter ces techniques √† vos besoins sp√©cifiques."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
