{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP : Introduction pratique aux mod√®les NLP et LLM\n",
    "\n",
    "## Objectifs du TP (1h30-2h)\n",
    "\n",
    "Dans ce TP, nous allons explorer :\n",
    "1. **La repr√©sentation du langage naturel** : tokenizers et encodage du texte\n",
    "2. **Les caract√©ristiques des mod√®les GPT** : pr√©diction de tokens, context window\n",
    "3. **L'impact du prompting et de la temp√©rature** sur les sorties du mod√®le\n",
    "4. **La diversit√© des mod√®les** disponibles sur Hugging Face\n",
    "\n",
    "---\n",
    "\n",
    "## Installation des d√©pendances\n",
    "\n",
    "Ex√©cutez la cellule suivante pour installer les biblioth√®ques n√©cessaires :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des biblioth√®ques\n",
    "!pip install transformers torch datasets sentencepiece sacremoses -q\n",
    "\n",
    "# Imports n√©cessaires\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    pipeline\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import display, HTML\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Biblioth√®ques install√©es et import√©es avec succ√®s!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Partie 1 : Repr√©senter le langage naturel - Les Tokenizers\n",
    "\n",
    "### 1.1 Introduction aux tokenizers\n",
    "\n",
    "Les tokenizers sont des outils essentiels qui convertissent le texte en tokens (unit√©s de base) que les mod√®les peuvent traiter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Textes d'exemple pour nos tests\n",
    "textes_exemples = [\n",
    "    \"Bonjour, comment allez-vous aujourd'hui?\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"‰∫∫Â∑•Êô∫ËÉΩÊ≠£Âú®ÊîπÂèò‰∏ñÁïå„ÄÇ\",  # Chinois : \"L'IA change le monde\"\n",
    "    \"ü§ñ Les emojis sont-ils bien tokenis√©s? üöÄ\",\n",
    "    \"import numpy as np\\nprint('Hello World!')\"\n",
    "]\n",
    "\n",
    "# Fonction utilitaire pour visualiser la tokenisation\n",
    "def visualiser_tokenisation(tokenizer, texte, nom_tokenizer):\n",
    "    tokens = tokenizer.tokenize(texte)\n",
    "    token_ids = tokenizer.encode(texte)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Tokenizer: {nom_tokenizer}\")\n",
    "    print(f\"Texte original: {texte}\")\n",
    "    print(f\"Nombre de tokens: {len(tokens)}\")\n",
    "    print(f\"Tokens: {tokens[:20]}{'...' if len(tokens) > 20 else ''}\")\n",
    "    print(f\"Token IDs: {token_ids[:20]}{'...' if len(token_ids) > 20 else ''}\")\n",
    "    \n",
    "    return len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Comparaison de diff√©rents tokenizers\n",
    "\n",
    "Nous allons comparer plusieurs tokenizers populaires :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement de diff√©rents tokenizers\n",
    "tokenizers = {\n",
    "    \"GPT-2\": AutoTokenizer.from_pretrained(\"gpt2\"),\n",
    "    \"BERT\": AutoTokenizer.from_pretrained(\"bert-base-uncased\"),\n",
    "    \"T5\": AutoTokenizer.from_pretrained(\"t5-small\"),\n",
    "    \"CamemBERT (Fran√ßais)\": AutoTokenizer.from_pretrained(\"camembert-base\")\n",
    "}\n",
    "\n",
    "# Analyse comparative\n",
    "resultats = {}\n",
    "for nom, tokenizer in tokenizers.items():\n",
    "    print(f\"\\n\\n{'#'*80}\\n{nom.upper()}\\n{'#'*80}\")\n",
    "    resultats[nom] = []\n",
    "    \n",
    "    for texte in textes_exemples:\n",
    "        try:\n",
    "            nb_tokens = visualiser_tokenisation(tokenizer, texte, nom)\n",
    "            resultats[nom].append(nb_tokens)\n",
    "        except:\n",
    "            print(f\"‚ùå Erreur avec le texte: {texte}\")\n",
    "            resultats[nom].append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù Question 1.1\n",
    "\n",
    "**Observez les r√©sultats ci-dessus. Que remarquez-vous concernant :**\n",
    "- Le nombre de tokens pour le m√™me texte selon les tokenizers ?\n",
    "- La gestion des caract√®res sp√©ciaux et emojis ?\n",
    "- La tokenisation du code Python ?\n",
    "\n",
    "**Votre r√©ponse :** *(Double-cliquez pour √©diter)*\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation graphique des r√©sultats\n",
    "import pandas as pd\n",
    "\n",
    "df_resultats = pd.DataFrame(resultats, index=[f\"Texte {i+1}\" for i in range(len(textes_exemples))])\n",
    "\n",
    "# Graphique\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "df_resultats.plot(kind='bar', ax=ax)\n",
    "ax.set_title(\"Nombre de tokens par tokenizer et par texte\")\n",
    "ax.set_xlabel(\"Textes\")\n",
    "ax.set_ylabel(\"Nombre de tokens\")\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nR√©sum√© statistique:\")\n",
    "print(df_resultats.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Exploration approfondie : Vocabulaire et tokens sp√©ciaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explorons le vocabulaire d'un tokenizer\n",
    "tokenizer_gpt2 = tokenizers[\"GPT-2\"]\n",
    "\n",
    "print(f\"Taille du vocabulaire GPT-2: {tokenizer_gpt2.vocab_size}\")\n",
    "print(f\"\\nTokens sp√©ciaux:\")\n",
    "print(f\"- Token de padding: {tokenizer_gpt2.pad_token}\")\n",
    "print(f\"- Token de d√©but: {tokenizer_gpt2.bos_token}\")\n",
    "print(f\"- Token de fin: {tokenizer_gpt2.eos_token}\")\n",
    "print(f\"- Token inconnu: {tokenizer_gpt2.unk_token}\")\n",
    "\n",
    "# Exemple de d√©codage\n",
    "texte_test = \"Intelligence artificielle\"\n",
    "tokens_ids = tokenizer_gpt2.encode(texte_test)\n",
    "texte_decode = tokenizer_gpt2.decode(tokens_ids)\n",
    "\n",
    "print(f\"\\nEncodage/D√©codage:\")\n",
    "print(f\"Texte original: '{texte_test}'\")\n",
    "print(f\"Token IDs: {tokens_ids}\")\n",
    "print(f\"Texte d√©cod√©: '{texte_decode}'\")\n",
    "print(f\"Correspondance parfaite: {texte_test == texte_decode}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù Exercice 1.1 : Analyse de tokenisation\n",
    "\n",
    "Modifiez le texte ci-dessous et observez comment la tokenisation change :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Modifiez ce texte pour tester diff√©rents cas\n",
    "mon_texte = \"L'intelligence artificielle r√©volutionne notre quotidien!\"\n",
    "\n",
    "# Testez avec diff√©rents tokenizers\n",
    "for nom, tokenizer in list(tokenizers.items())[:2]:  # On teste avec les 2 premiers\n",
    "    tokens = tokenizer.tokenize(mon_texte)\n",
    "    print(f\"\\n{nom}:\")\n",
    "    print(f\"  Tokens ({len(tokens)}): {tokens}\")\n",
    "    \n",
    "    # Reconstruction token par token\n",
    "    print(\"  Reconstruction: \", end=\"\")\n",
    "    for token in tokens:\n",
    "        print(f\"[{token}]\", end=\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Partie 2 : Comprendre les mod√®les GPT\n",
    "\n",
    "### 2.1 Chargement d'un mod√®le GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement du mod√®le GPT-2 small\n",
    "print(\"Chargement du mod√®le GPT-2... (cela peut prendre un moment)\")\n",
    "\n",
    "model_name = \"gpt2\"  # Vous pouvez essayer \"gpt2-medium\" si vous avez plus de RAM\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Configuration du token de padding\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Informations sur le mod√®le\n",
    "print(f\"\\n‚úÖ Mod√®le charg√©: {model_name}\")\n",
    "print(f\"Nombre de param√®tres: {model.num_parameters():,}\")\n",
    "print(f\"Taille du contexte maximum: {model.config.n_ctx} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 G√©n√©ration de texte : Pr√©diction de tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generer_texte(prompt, max_length=50, temperature=1.0, num_return_sequences=1):\n",
    "    \"\"\"\n",
    "    G√©n√®re du texte √† partir d'un prompt\n",
    "    \"\"\"\n",
    "    # Encodage du prompt\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    # G√©n√©ration\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            do_sample=True\n",
    "        )\n",
    "    \n",
    "    # D√©codage et affichage\n",
    "    for i, output in enumerate(outputs):\n",
    "        texte_genere = tokenizer.decode(output, skip_special_tokens=True)\n",
    "        print(f\"\\nG√©n√©ration {i+1}:\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(texte_genere)\n",
    "\n",
    "# Test de g√©n√©ration\n",
    "prompt_test = \"L'intelligence artificielle est\"\n",
    "print(f\"Prompt: '{prompt_test}'\\n\")\n",
    "generer_texte(prompt_test, max_length=50, temperature=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Importance du bon tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©monstration : utiliser le mauvais tokenizer\n",
    "print(\"üîç Exp√©rience : Que se passe-t-il si on utilise le mauvais tokenizer?\\n\")\n",
    "\n",
    "# Tokenizer correct (GPT-2)\n",
    "prompt = \"Paris is the capital of\"\n",
    "inputs_correct = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Tokenizer incorrect (BERT)\n",
    "tokenizer_bert = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "inputs_incorrect = tokenizer_bert.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "print(f\"Prompt: '{prompt}'\")\n",
    "print(f\"\\nTokens GPT-2 (correct): {tokenizer.tokenize(prompt)}\")\n",
    "print(f\"Token IDs GPT-2: {inputs_correct[0].tolist()}\")\n",
    "print(f\"\\nTokens BERT (incorrect): {tokenizer_bert.tokenize(prompt)}\")\n",
    "print(f\"Token IDs BERT: {inputs_incorrect[0].tolist()}\")\n",
    "\n",
    "# G√©n√©ration avec le bon tokenizer\n",
    "with torch.no_grad():\n",
    "    output_correct = model.generate(inputs_correct, max_length=20, temperature=0.1)\n",
    "    texte_correct = tokenizer.decode(output_correct[0], skip_special_tokens=True)\n",
    "    \n",
    "print(f\"\\n‚úÖ G√©n√©ration avec le bon tokenizer: '{texte_correct}'\")\n",
    "\n",
    "# Tentative avec les IDs du mauvais tokenizer (attention aux erreurs!)\n",
    "try:\n",
    "    # On va cr√©er des IDs al√©atoires pour simuler\n",
    "    fake_ids = torch.randint(0, tokenizer.vocab_size, (1, len(inputs_incorrect[0])))\n",
    "    with torch.no_grad():\n",
    "        output_incorrect = model.generate(fake_ids, max_length=20, temperature=0.1)\n",
    "        texte_incorrect = tokenizer.decode(output_incorrect[0], skip_special_tokens=True)\n",
    "    print(f\"‚ùå G√©n√©ration avec mauvais tokenizer: '{texte_incorrect}'\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur avec le mauvais tokenizer: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù Question 2.1\n",
    "\n",
    "**Pourquoi est-il crucial d'utiliser le tokenizer correspondant au mod√®le?**\n",
    "\n",
    "**Votre r√©ponse :** *(Double-cliquez pour √©diter)*\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Limites du contexte (Context Window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©monstration de la limite de contexte\n",
    "print(f\"üîç Limite de contexte du mod√®le: {model.config.n_ctx} tokens\\n\")\n",
    "\n",
    "# Cr√©ons un texte tr√®s long\n",
    "texte_court = \"Ceci est un exemple. \" * 10\n",
    "texte_long = \"Ceci est un exemple. \" * 100\n",
    "\n",
    "tokens_court = tokenizer.encode(texte_court)\n",
    "tokens_long = tokenizer.encode(texte_long)\n",
    "\n",
    "print(f\"Texte court: {len(tokens_court)} tokens\")\n",
    "print(f\"Texte long: {len(tokens_long)} tokens\")\n",
    "print(f\"D√©passe la limite? {len(tokens_long) > model.config.n_ctx}\")\n",
    "\n",
    "# Visualisation de ce qui se passe avec un texte trop long\n",
    "if len(tokens_long) > model.config.n_ctx:\n",
    "    print(f\"\\n‚ö†Ô∏è Le texte long ({len(tokens_long)} tokens) d√©passe la limite!\")\n",
    "    print(f\"Le mod√®le ne peut traiter que les {model.config.n_ctx} premiers tokens.\")\n",
    "    \n",
    "    # Tronquons pour la d√©monstration\n",
    "    tokens_tronques = tokens_long[:model.config.n_ctx]\n",
    "    texte_tronque = tokenizer.decode(tokens_tronques)\n",
    "    print(f\"\\nTexte tronqu√© (d√©but): {texte_tronque[:100]}...\")\n",
    "    print(f\"Texte tronqu√© (fin): ...{texte_tronque[-100:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Date limite d'entra√Ænement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test des connaissances temporelles du mod√®le\n",
    "prompts_temporels = [\n",
    "    \"The president of the United States in 2019 was\",\n",
    "    \"The COVID-19 pandemic started in\",\n",
    "    \"The latest iPhone model in 2023 is\",\n",
    "    \"The winner of the 2022 FIFA World Cup was\"\n",
    "]\n",
    "\n",
    "print(\"üïê Test des connaissances temporelles du mod√®le\\n\")\n",
    "print(\"Note: GPT-2 a √©t√© entra√Æn√© sur des donn√©es jusqu'en 2019\\n\")\n",
    "\n",
    "for prompt in prompts_temporels:\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            max_length=len(inputs[0]) + 20,\n",
    "            temperature=0.1,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    resultat = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(f\"Q: {prompt}\")\n",
    "    print(f\"R: {resultat}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù Exercice 2.1 : Explorer les limites du mod√®le\n",
    "\n",
    "Testez le mod√®le avec vos propres prompts pour identifier ses limites temporelles :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Ajoutez vos propres prompts pour tester les connaissances du mod√®le\n",
    "mes_prompts = [\n",
    "    \"En 2018, le champion du monde de football √©tait\",\n",
    "    # Ajoutez vos prompts ici\n",
    "]\n",
    "\n",
    "for prompt in mes_prompts:\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(inputs, max_length=50, temperature=0.3, pad_token_id=tokenizer.eos_token_id)\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"R√©ponse: {tokenizer.decode(outputs[0], skip_special_tokens=True)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Partie 3 : Impact du prompting et de la temp√©rature\n",
    "\n",
    "### 3.1 Exploration de la temp√©rature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour visualiser l'impact de la temp√©rature\n",
    "def explorer_temperature(prompt, temperatures=[0.1, 0.5, 0.8, 1.0, 1.5]):\n",
    "    print(f\"Prompt: '{prompt}'\\n\")\n",
    "    \n",
    "    resultats = {}\n",
    "    \n",
    "    for temp in temperatures:\n",
    "        print(f\"\\nüå°Ô∏è Temp√©rature = {temp}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "        generations = []\n",
    "        \n",
    "        # G√©n√©rer plusieurs fois pour voir la variabilit√©\n",
    "        for i in range(3):\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "    inputs,\n",
    "                    max_length=50,\n",
    "                    temperature=temp,\n",
    "                    do_sample=True,\n",
    "                    pad_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "            \n",
    "            texte = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            generations.append(texte)\n",
    "            print(f\"Essai {i+1}: {texte}\")\n",
    "        \n",
    "        resultats[temp] = generations\n",
    "    \n",
    "    return resultats\n",
    "\n",
    "# Test avec diff√©rentes temp√©ratures\n",
    "resultats_temp = explorer_temperature(\"Une recette de cuisine innovante:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù Question 3.1\n",
    "\n",
    "**Observez l'impact de la temp√©rature sur les g√©n√©rations:**\n",
    "- Que se passe-t-il avec une temp√©rature basse (0.1) ?\n",
    "- Que se passe-t-il avec une temp√©rature √©lev√©e (1.5) ?\n",
    "- Quelle temp√©rature recommanderiez-vous pour des cas d'usage cr√©atifs vs factuels ?\n",
    "\n",
    "**Votre r√©ponse :** *(Double-cliquez pour √©diter)*\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 L'art du prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison de diff√©rents styles de prompts\n",
    "tache = \"expliquer ce qu'est le machine learning\"\n",
    "\n",
    "prompts_styles = {\n",
    "    \"Basique\": \"Le machine learning est\",\n",
    "    \n",
    "    \"Avec contexte\": \"En tant que professeur d'informatique, j'explique √† mes √©tudiants que le machine learning est\",\n",
    "    \n",
    "    \"Avec exemple\": \"Le machine learning est comme apprendre √† faire du v√©lo. Au d√©but,\",\n",
    "    \n",
    "    \"Structur√©\": \"D√©finition du Machine Learning:\\n1.\",\n",
    "    \n",
    "    \"Conversation\": \"√âtudiant: Qu'est-ce que le machine learning?\\nProfesseur: Excellente question! Le machine learning est\"\n",
    "}\n",
    "\n",
    "print(\"üéØ Comparaison de diff√©rents styles de prompting\\n\")\n",
    "\n",
    "for style, prompt in prompts_styles.items():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Style: {style}\")\n",
    "    print(f\"Prompt: '{prompt}'\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            max_length=100,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            do_sample=True\n",
    "        )\n",
    "    \n",
    "    resultat = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(resultat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù Exercice 3.1 : Cr√©ez vos propres prompts\n",
    "\n",
    "Exp√©rimentez avec diff√©rents prompts pour une t√¢che de votre choix :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Cr√©ez 3 prompts diff√©rents pour accomplir la m√™me t√¢che\n",
    "ma_tache = \"g√©n√©rer une histoire courte sur un robot\"\n",
    "\n",
    "mes_prompts = {\n",
    "    \"Prompt 1\": \"Il √©tait une fois un robot qui\",\n",
    "    \"Prompt 2\": \"Journal intime d'un robot, jour 1:\",\n",
    "    \"Prompt 3\": \"Breaking news: Un robot extraordinaire vient de\"\n",
    "}\n",
    "\n",
    "# Testez vos prompts\n",
    "temperature_choisie = 0.8  # Modifiez cette valeur\n",
    "\n",
    "for nom, prompt in mes_prompts.items():\n",
    "    print(f\"\\n{nom}: '{prompt}'\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            max_length=80,\n",
    "            temperature=temperature_choisie,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            do_sample=True\n",
    "        )\n",
    "    \n",
    "    print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Partie 4 : La diversit√© des mod√®les sur Hugging Face\n",
    "\n",
    "### 4.1 Pipeline : Interface simple pour diff√©rentes t√¢ches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pr√©sentation des diff√©rentes t√¢ches disponibles\n",
    "print(\"üéØ Exemples de t√¢ches NLP disponibles sur Hugging Face:\\n\")\n",
    "\n",
    "taches_exemples = [\n",
    "    \"text-generation\",\n",
    "    \"sentiment-analysis\", \n",
    "    \"translation\",\n",
    "    \"summarization\",\n",
    "    \"question-answering\",\n",
    "    \"zero-shot-classification\",\n",
    "    \"text2text-generation\"\n",
    "]\n",
    "\n",
    "for tache in taches_exemples:\n",
    "    print(f\"- {tache}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Analyse de sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline d'analyse de sentiment\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "# Textes √† analyser\n",
    "textes_sentiment = [\n",
    "    \"J'adore ce cours sur le NLP, c'est passionnant!\",\n",
    "    \"Cette exp√©rience est vraiment d√©cevante.\",\n",
    "    \"Le temps est nuageux aujourd'hui.\",\n",
    "    \"This transformer model is absolutely amazing! üöÄ\",\n",
    "    \"Je ne sais pas trop quoi en penser, c'est mitig√©.\"\n",
    "]\n",
    "\n",
    "print(\"üòäüòêüò¢ Analyse de sentiment\\n\")\n",
    "\n",
    "resultats_sentiment = sentiment_analyzer(textes_sentiment)\n",
    "\n",
    "for texte, resultat in zip(textes_sentiment, resultats_sentiment):\n",
    "    emoji = \"üòä\" if resultat['label'] == 'POSITIVE' else \"üò¢\"\n",
    "    print(f\"{emoji} Texte: '{texte}'\")\n",
    "    print(f\"   ‚Üí Sentiment: {resultat['label']} (confiance: {resultat['score']:.2%})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Classification zero-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification zero-shot : classifier sans entra√Ænement sp√©cifique\n",
    "classifier = pipeline(\"zero-shot-classification\")\n",
    "\n",
    "# Textes √† classifier\n",
    "textes_classification = [\n",
    "    \"Tesla annonce une nouvelle voiture √©lectrique autonome\",\n",
    "    \"Le PSG remporte le match 3-0 contre Lyon\",\n",
    "    \"Nouvelle d√©couverte scientifique sur les trous noirs\",\n",
    "    \"Apple d√©voile l'iPhone 15 avec des fonctionnalit√©s r√©volutionnaires\"\n",
    "]\n",
    "\n",
    "# Cat√©gories possibles\n",
    "categories = [\"sport\", \"technologie\", \"science\", \"politique\", \"√©conomie\"]\n",
    "\n",
    "print(\"üè∑Ô∏è Classification zero-shot\\n\")\n",
    "\n",
    "for texte in textes_classification:\n",
    "    resultat = classifier(texte, candidate_labels=categories)\n",
    "    \n",
    "    print(f\"Texte: '{texte}'\")\n",
    "    print(f\"Classifications:\")\n",
    "    for label, score in zip(resultat['labels'][:3], resultat['scores'][:3]):\n",
    "        print(f\"  - {label}: {score:.2%}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 G√©n√©ration de r√©sum√©s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline de r√©sum√©\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "# Texte long √† r√©sumer\n",
    "article = \"\"\"\n",
    "L'intelligence artificielle (IA) transforme rapidement notre monde. \n",
    "Des assistants vocaux dans nos maisons aux syst√®mes de recommandation sur nos plateformes de streaming, \n",
    "l'IA est omnipr√©sente. Les r√©centes avanc√©es en apprentissage profond, notamment avec les transformers, \n",
    "ont permis des progr√®s spectaculaires dans le traitement du langage naturel. \n",
    "Ces mod√®les peuvent maintenant comprendre et g√©n√©rer du texte avec une qualit√© impressionnante, \n",
    "ouvrant de nouvelles possibilit√©s dans de nombreux domaines comme la traduction automatique, \n",
    "la g√©n√©ration de contenu, et l'analyse de sentiment. Cependant, ces avanc√©es soul√®vent aussi \n",
    "des questions √©thiques importantes concernant la vie priv√©e, les biais algorithmiques, \n",
    "et l'impact sur l'emploi.\n",
    "\"\"\"\n",
    "\n",
    "print(\"üìù G√©n√©ration de r√©sum√©\\n\")\n",
    "print(\"Article original:\")\n",
    "print(article)\n",
    "print(\"\\nR√©sum√© g√©n√©r√©:\")\n",
    "\n",
    "resume = summarizer(article, max_length=80, min_length=30, do_sample=False)\n",
    "print(resume[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Questions-R√©ponses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline de questions-r√©ponses\n",
    "qa_pipeline = pipeline(\"question-answering\")\n",
    "\n",
    "# Contexte\n",
    "contexte = \"\"\"\n",
    "Hugging Face est une entreprise franco-am√©ricaine fond√©e en 2016 par Cl√©ment Delangue, Julien Chaumond et Thomas Wolf.\n",
    "L'entreprise est surtout connue pour sa biblioth√®que Transformers, qui fournit des architectures de pointe \n",
    "pour le traitement du langage naturel. La plateforme Hugging Face Hub h√©berge plus de 100 000 mod√®les \n",
    "pr√©-entra√Æn√©s que les d√©veloppeurs peuvent utiliser gratuitement. L'entreprise a lev√© 235 millions de dollars \n",
    "en 2023, atteignant une valorisation de 4,5 milliards de dollars.\n",
    "\"\"\"\n",
    "\n",
    "# Questions\n",
    "questions = [\n",
    "    \"Quand a √©t√© fond√©e Hugging Face?\",\n",
    "    \"Qui sont les fondateurs?\",\n",
    "    \"Combien de mod√®les sont h√©berg√©s sur le Hub?\",\n",
    "    \"Quelle est la valorisation de l'entreprise?\"\n",
    "]\n",
    "\n",
    "print(\"‚ùì Questions-R√©ponses\\n\")\n",
    "print(f\"Contexte: {contexte}\\n\")\n",
    "\n",
    "for question in questions:\n",
    "    reponse = qa_pipeline(question=question, context=contexte)\n",
    "    print(f\"Q: {question}\")\n",
    "    print(f\"R: {reponse['answer']} (confiance: {reponse['score']:.2%})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù Exercice 4.1 : Explorer d'autres mod√®les\n",
    "\n",
    "Choisissez un mod√®le sp√©cialis√© sur Hugging Face et testez-le :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Explorez un autre mod√®le de votre choix\n",
    "# Exemples de mod√®les int√©ressants:\n",
    "# - \"Helsinki-NLP/opus-mt-fr-en\" pour la traduction fran√ßais‚Üíanglais\n",
    "# - \"camembert-base\" pour du NLP en fran√ßais\n",
    "# - \"microsoft/DialoGPT-medium\" pour la conversation\n",
    "\n",
    "# Votre code ici\n",
    "mon_modele = \"Helsinki-NLP/opus-mt-fr-en\"  # Changez ce mod√®le\n",
    "ma_tache = \"translation_fr_to_en\"  # Adaptez la t√¢che\n",
    "\n",
    "# Cr√©ez votre pipeline et testez-le\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conclusion et points cl√©s √† retenir\n",
    "\n",
    "### üéØ Ce que nous avons appris :\n",
    "\n",
    "1. **Tokenisation** :\n",
    "   - Les tokenizers transforment le texte en unit√©s que les mod√®les peuvent traiter\n",
    "   - Diff√©rents tokenizers ont diff√©rentes strat√©gies (BPE, WordPiece, etc.)\n",
    "   - Il est crucial d'utiliser le bon tokenizer avec le bon mod√®le\n",
    "\n",
    "2. **Mod√®les GPT** :\n",
    "   - Fonctionnent par pr√©diction du token suivant\n",
    "   - Ont une limite de contexte (context window)\n",
    "   - Leurs connaissances sont limit√©es √† leur date d'entra√Ænement\n",
    "\n",
    "3. **Param√®tres de g√©n√©ration** :\n",
    "   - La temp√©rature contr√¥le la cr√©ativit√©/al√©atoire\n",
    "   - Le prompting est un art qui influence grandement les r√©sultats\n",
    "\n",
    "4. **√âcosyst√®me Hugging Face** :\n",
    "   - Grande vari√©t√© de mod√®les pour diff√©rentes t√¢ches\n",
    "   - Pipelines pour une utilisation simplifi√©e\n",
    "   - Mod√®les sp√©cialis√©s vs mod√®les g√©n√©ralistes\n",
    "\n",
    "### üìö Pour aller plus loin :\n",
    "\n",
    "- Documentation Hugging Face : https://huggingface.co/docs\n",
    "- Cours sur les Transformers : https://huggingface.co/course\n",
    "- Model Hub : https://huggingface.co/models\n",
    "- Papers with Code : https://paperswithcode.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° Mini-projet final (optionnel)\n",
    "\n",
    "Cr√©ez une petite application qui combine plusieurs mod√®les :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple : Assistant multilingue\n",
    "def assistant_multilingue(texte, langue_source=\"fr\", langue_cible=\"en\"):\n",
    "    \"\"\"\n",
    "    1. D√©tecte le sentiment\n",
    "    2. Traduit le texte\n",
    "    3. G√©n√®re une r√©ponse appropri√©e\n",
    "    \"\"\"\n",
    "    print(f\"üì• Texte re√ßu: '{texte}'\\n\")\n",
    "    \n",
    "    # √âtape 1: Analyse de sentiment\n",
    "    sentiment = sentiment_analyzer(texte)[0]\n",
    "    print(f\"üòä Sentiment d√©tect√©: {sentiment['label']} ({sentiment['score']:.2%})\")\n",
    "    \n",
    "    # √âtape 2: Traduction (si n√©cessaire)\n",
    "    if langue_source != langue_cible:\n",
    "        translator = pipeline(f\"translation_{langue_source}_to_{langue_cible}\")\n",
    "        traduction = translator(texte)[0]['translation_text']\n",
    "        print(f\"üåç Traduction: '{traduction}'\")\n",
    "    \n",
    "    # √âtape 3: G√©n√©ration de r√©ponse bas√©e sur le sentiment\n",
    "    if sentiment['label'] == 'POSITIVE':\n",
    "        prompt = \"Thank you for your positive feedback! I'm glad\"\n",
    "    else:\n",
    "        prompt = \"I understand your concern. Let me help you by\"\n",
    "    \n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(inputs, max_length=50, temperature=0.7, pad_token_id=tokenizer.eos_token_id)\n",
    "    reponse = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"\\nüí¨ R√©ponse g√©n√©r√©e: {reponse}\")\n",
    "    \n",
    "    return reponse\n",
    "\n",
    "# Test\n",
    "assistant_multilingue(\"Ce TP √©tait vraiment int√©ressant et bien structur√©!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ F√©licitations!\n",
    "\n",
    "Vous avez termin√© ce TP d'introduction aux mod√®les NLP et LLM. Vous avez maintenant les bases pour:\n",
    "\n",
    "- Comprendre comment les mod√®les de langage traitent le texte\n",
    "- Utiliser et configurer des mod√®les pr√©-entra√Æn√©s\n",
    "- Explorer l'√©cosyst√®me riche de Hugging Face\n",
    "- Cr√©er vos propres applications NLP\n",
    "\n",
    "N'h√©sitez pas √† exp√©rimenter davantage avec les diff√©rents mod√®les et param√®tres!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
